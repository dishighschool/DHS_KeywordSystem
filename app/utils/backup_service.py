"""å‚™ä»½æœå‹™ - ç®¡ç†ç³»çµ±è³‡æ–™å‚™ä»½"""
from __future__ import annotations

import gzip
import json
import os
import shutil
from datetime import datetime, timedelta
from pathlib import Path
from typing import TYPE_CHECKING, Optional

from flask import current_app

if TYPE_CHECKING:
    from sqlalchemy.orm import Session

from ..extensions import db
from ..models import (
    AnnouncementBanner,
    EditLog,
    FooterSocialLink,
    KeywordAlias,
    KeywordCategory,
    KeywordGoalItem,
    KeywordGoalList,
    LearningKeyword,
    NavigationLink,
    Role,
    SiteSetting,
    SiteSettingKey,
    SystemBackup,
    User,
    YouTubeVideo,
)


class BackupService:
    """ç³»çµ±å‚™ä»½æœå‹™"""

    # å‚™ä»½ç›®éŒ„
    BACKUP_DIR = Path(__file__).parent.parent.parent / "backups"

    # ä¿ç•™å‚™ä»½å¤©æ•¸
    RETENTION_DAYS = 30

    # å‚™ä»½æª”æ¡ˆå­—é¦–
    BACKUP_PREFIX = "system_backup"

    # Discord Webhook ä¸Šå‚³é™åˆ¶ (Discord API ä¸Šé™ç‚º 25 MB)
    DISCORD_UPLOAD_LIMIT = 25 * 1024 * 1024

    def __init__(self, session=None):
        """åˆå§‹åŒ–å‚™ä»½æœå‹™"""
        self.session = session or db.session
        self._ensure_backup_dir()

    @classmethod
    def _ensure_backup_dir(cls) -> Path:
        """ç¢ºä¿å‚™ä»½ç›®éŒ„å­˜åœ¨"""
        backup_dir = cls.BACKUP_DIR
        backup_dir.mkdir(parents=True, exist_ok=True)
        return backup_dir

    @classmethod
    def create_backup(
        cls,
        session=None,
        created_by=None,
        backup_type: str = "auto",
        description: str | None = None,
    ) -> Optional[SystemBackup]:
        """
        å»ºç«‹ç³»çµ±å‚™ä»½

        Args:
            session: è³‡æ–™åº«æœƒè©±
            created_by: å»ºç«‹è€… ID
            backup_type: å‚™ä»½é¡å‹ ('auto' æˆ– 'manual')
            description: å‚™ä»½èªªæ˜

        Returns:
            SystemBackup ç‰©ä»¶æˆ– None (å¦‚æœå»ºç«‹å¤±æ•—)
        """
        if session is None:
            session = db.session

        try:
            service = cls(session)

            # æ”¶é›†æ‰€æœ‰è³‡æ–™
            data = {
                "export_info": {
                    "version": "1.0",
                    "exported_at": datetime.utcnow().isoformat(),
                    "exported_by_id": created_by,
                },
                "users": [],
                "categories": [],
                "keywords": [],
                "aliases": [],
                "videos": [],
                "navigation_links": [],
                "footer_links": [],
                "announcements": [],
                "site_settings": [],
                "goal_lists": [],
                "goal_items": [],
            }

            # åŒ¯å‡ºç”¨æˆ¶ (ä¸åŒ…å«æ•æ„Ÿè³‡è¨Š)
            for user in session.query(User).all():
                data["users"].append({
                    "id": user.id,
                    "discord_id": user.discord_id,
                    "username": user.username,
                    "avatar_hash": user.avatar_hash,
                    "role": user.role.value,
                    "is_active": user.is_active,
                    "created_at": user.created_at.isoformat(),
                })

            # åŒ¯å‡ºåˆ†é¡
            for category in session.query(KeywordCategory).order_by(KeywordCategory.position).all():
                data["categories"].append({
                    "id": category.id,
                    "name": category.name,
                    "slug": category.slug,
                    "description": category.description,
                    "position": category.position,
                    "icon": category.icon,
                    "is_public": category.is_public,
                    "created_at": category.created_at.isoformat(),
                })

            # åŒ¯å‡ºé—œéµå­—
            for keyword in session.query(LearningKeyword).order_by(
                LearningKeyword.category_id, LearningKeyword.position
            ).all():
                data["keywords"].append({
                    "id": keyword.id,
                    "title": keyword.title,
                    "slug": keyword.slug,
                    "description_markdown": keyword.description_markdown,
                    "position": keyword.position,
                    "is_public": keyword.is_public,
                    "view_count": keyword.view_count,
                    "seo_content": keyword.seo_content,
                    "seo_auto_generate": keyword.seo_auto_generate,
                    "category_id": keyword.category_id,
                    "author_id": keyword.author_id,
                    "created_at": keyword.created_at.isoformat(),
                    "updated_at": keyword.updated_at.isoformat(),
                })

            # åŒ¯å‡ºåˆ¥å
            for alias in session.query(KeywordAlias).all():
                data["aliases"].append({
                    "id": alias.id,
                    "keyword_id": alias.keyword_id,
                    "title": alias.title,
                    "slug": alias.slug,
                    "created_at": alias.created_at.isoformat(),
                })

            # åŒ¯å‡ºå½±ç‰‡
            for video in session.query(YouTubeVideo).all():
                data["videos"].append({
                    "id": video.id,
                    "keyword_id": video.keyword_id,
                    "title": video.title,
                    "url": video.url,
                    "created_at": video.created_at.isoformat(),
                })

            # åŒ¯å‡ºå°èˆªé€£çµ
            for nav in session.query(NavigationLink).order_by(NavigationLink.position).all():
                data["navigation_links"].append({
                    "id": nav.id,
                    "label": nav.label,
                    "url": nav.url,
                    "icon": nav.icon,
                    "position": nav.position,
                    "created_at": nav.created_at.isoformat(),
                })

            # åŒ¯å‡ºåº•éƒ¨é€£çµ
            for footer in session.query(FooterSocialLink).order_by(FooterSocialLink.position).all():
                data["footer_links"].append({
                    "id": footer.id,
                    "label": footer.label,
                    "url": footer.url,
                    "icon": footer.icon,
                    "position": footer.position,
                    "created_at": footer.created_at.isoformat(),
                })

            # åŒ¯å‡ºå…¬å‘Šæ©«å¹…
            for announcement in session.query(AnnouncementBanner).order_by(
                AnnouncementBanner.position
            ).all():
                data["announcements"].append({
                    "id": announcement.id,
                    "text": announcement.text,
                    "url": announcement.url,
                    "icon": announcement.icon,
                    "is_active": announcement.is_active,
                    "position": announcement.position,
                    "created_at": announcement.created_at.isoformat(),
                })

            # åŒ¯å‡ºç¶²ç«™è¨­å®š
            for setting in session.query(SiteSetting).all():
                data["site_settings"].append({
                    "key": setting.key,
                    "value": setting.value,
                    "updated_at": setting.updated_at.isoformat(),
                })

            # åŒ¯å‡ºç›®æ¨™æ¸…å–®
            for goal_list in session.query(KeywordGoalList).all():
                data["goal_lists"].append({
                    "id": goal_list.id,
                    "name": goal_list.name,
                    "description": goal_list.description,
                    "category_name": goal_list.category_name,
                    "is_active": goal_list.is_active,
                    "created_by": goal_list.created_by,
                    "created_at": goal_list.created_at.isoformat(),
                })

            # åŒ¯å‡ºç›®æ¨™é …ç›®
            for goal_item in session.query(KeywordGoalItem).all():
                data["goal_items"].append({
                    "id": goal_item.id,
                    "goal_list_id": goal_item.goal_list_id,
                    "title": goal_item.title,
                    "position": goal_item.position,
                    "is_completed": goal_item.is_completed,
                    "keyword_id": goal_item.keyword_id,
                    "completed_by": goal_item.completed_by,
                    "completed_at": goal_item.completed_at.isoformat()
                    if goal_item.completed_at
                    else None,
                    "created_at": goal_item.created_at.isoformat(),
                })

            # ç”Ÿæˆæª”æ¡ˆåç¨±å’Œè·¯å¾‘
            timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S_%f")[:-3]  # åŒ…å«æ¯«ç§’
            filename = f"{cls.BACKUP_PREFIX}_{timestamp}.json.gz"
            filepath = cls.BACKUP_DIR / filename

            # å¯«å…¥å£“ç¸®æª”æ¡ˆ
            json_data = json.dumps(data, ensure_ascii=False, indent=2)
            with gzip.open(filepath, "wt", encoding="utf-8") as f:
                f.write(json_data)

            # å–å¾—æª”æ¡ˆå¤§å°
            file_size = filepath.stat().st_size

            # è¨˜éŒ„åˆ°è³‡æ–™åº«
            backup_record = SystemBackup(
                filename=filename,
                filepath=str(filepath),
                file_size=file_size,
                backup_type=backup_type,
                created_by=created_by,
                description=description,
                is_compressed=True,
            )
            session.add(backup_record)
            session.commit()

            current_app.logger.info(
                f"Backup created: {filename} ({service._format_size(file_size)})"
            )

            # å‚³é€å‚™ä»½é€šçŸ¥åˆ° Discordï¼ˆå¦‚æœæœ‰è¨­å®š webhookï¼‰
            cls._notify_discord_webhook(backup_record)

            return backup_record

        except Exception as e:
            current_app.logger.error(f"Backup creation failed: {e}")
            return None

    @classmethod
    def get_backup_list(
        cls,
        session=None,
        limit: int | None = None,
    ) -> list:
        """
        å–å¾—å‚™ä»½åˆ—è¡¨

        Args:
            session: è³‡æ–™åº«æœƒè©±
            limit: é™åˆ¶æ•¸é‡

        Returns:
            å‚™ä»½åˆ—è¡¨ï¼ŒæŒ‰å»ºç«‹æ™‚é–“å€’åº
        """
        if session is None:
            session = db.session

        query = session.query(SystemBackup).order_by(SystemBackup.created_at.desc())

        if limit:
            query = query.limit(limit)

        return query.all()

    @classmethod
    def delete_backup(cls, backup_id: int, session=None) -> bool:
        """
        åˆªé™¤å‚™ä»½

        Args:
            backup_id: å‚™ä»½ ID
            session: è³‡æ–™åº«æœƒè©±

        Returns:
            æ˜¯å¦åˆªé™¤æˆåŠŸ
        """
        if session is None:
            session = db.session

        try:
            backup = session.query(SystemBackup).get(backup_id)
            if not backup:
                return False

            # åˆªé™¤æª”æ¡ˆ
            filepath = Path(backup.filepath)
            if filepath.exists():
                filepath.unlink()

            # åˆªé™¤è¨˜éŒ„
            session.delete(backup)
            session.commit()

            current_app.logger.info(f"Backup deleted: {backup.filename}")
            return True

        except Exception as e:
            current_app.logger.error(f"Backup deletion failed: {e}")
            return False

    @classmethod
    def cleanup_old_backups(
        cls,
        retention_days: int | None = None,
        session=None,
    ) -> int:
        """
        æ¸…ç†èˆŠå‚™ä»½ï¼ˆè¶…éä¿ç•™å¤©æ•¸ï¼‰

        Args:
            retention_days: ä¿ç•™å¤©æ•¸ï¼ˆé è¨­ç‚º RETENTION_DAYSï¼‰
            session: è³‡æ–™åº«æœƒè©±

        Returns:
            åˆªé™¤çš„å‚™ä»½æ•¸é‡
        """
        if retention_days is None:
            retention_days = cls.RETENTION_DAYS

        if session is None:
            session = db.session

        try:
            cutoff_date = datetime.utcnow() - timedelta(days=retention_days)
            old_backups = session.query(SystemBackup).filter(
                SystemBackup.created_at < cutoff_date
            ).all()

            deleted_count = 0

            for backup in old_backups:
                try:
                    # åˆªé™¤æª”æ¡ˆ
                    filepath = Path(backup.filepath)
                    if filepath.exists():
                        filepath.unlink()

                    # åˆªé™¤è¨˜éŒ„
                    session.delete(backup)
                    deleted_count += 1
                except Exception as e:
                    current_app.logger.error(f"Failed to delete backup {backup.filename}: {e}")

            session.commit()

            if deleted_count > 0:
                current_app.logger.info(f"Cleaned up {deleted_count} old backups")

            return deleted_count

        except Exception as e:
            current_app.logger.error(f"Backup cleanup failed: {e}")
            return 0

    @classmethod
    def get_backup_by_id(
        cls, backup_id: int, session=None
    ) -> Optional[SystemBackup]:
        """å–å¾—æŒ‡å®š ID çš„å‚™ä»½"""
        if session is None:
            session = db.session

        return session.query(SystemBackup).get(backup_id)

    @classmethod
    def get_backup_stats(cls, session=None) -> dict:
        """
        å–å¾—å‚™ä»½çµ±è¨ˆè³‡è¨Š

        Returns:
            å‚™ä»½çµ±è¨ˆè³‡è¨Šå­—å…¸
        """
        if session is None:
            session = db.session

        try:
            backups = session.query(SystemBackup).all()

            total_size = sum(b.file_size for b in backups)
            auto_backups = sum(1 for b in backups if b.backup_type == "auto")
            manual_backups = sum(1 for b in backups if b.backup_type == "manual")

            return {
                "total_backups": len(backups),
                "auto_backups": auto_backups,
                "manual_backups": manual_backups,
                "total_size": total_size,
                "total_size_formatted": cls._format_size(total_size),
                "oldest_backup": min((b.created_at for b in backups), default=None),
                "newest_backup": max((b.created_at for b in backups), default=None),
            }
        except Exception as e:
            current_app.logger.error(f"Failed to get backup stats: {e}")
            return {
                "total_backups": 0,
                "auto_backups": 0,
                "manual_backups": 0,
                "total_size": 0,
                "total_size_formatted": "0 B",
            }

    @staticmethod
    def _format_size(size: float) -> str:
        """æ ¼å¼åŒ–æª”æ¡ˆå¤§å°"""
        size = float(size)
        for unit in ["B", "KB", "MB", "GB"]:
            if size < 1024:
                return f"{size:.2f} {unit}"
            size /= 1024
        return f"{size:.2f} TB"

    @classmethod
    def _notify_discord_webhook(cls, backup: SystemBackup) -> None:
        """å°‡å‚™ä»½è³‡è¨Šæ¨é€åˆ° Discord Webhookï¼ˆå¦‚æœå·²è¨­å®šï¼‰ã€‚"""
        try:
            webhook_url = SiteSetting.get(SiteSettingKey.BACKUP_DISCORD_WEBHOOK_URL)
        except Exception as exc:  # pragma: no cover - defensive guard
            current_app.logger.warning(f"Failed to load Discord webhook setting: {exc}")
            return

        if not webhook_url:
            return

        normalized_url = webhook_url.strip()
        if not normalized_url.startswith("https://discord.com/api/webhooks/") and not normalized_url.startswith(
            "https://discordapp.com/api/webhooks/"
        ):
            current_app.logger.warning("Discord webhook URL æ ¼å¼ä¸æ­£ç¢ºï¼Œå·²ç•¥éé€šçŸ¥")
            return

        file_path = Path(backup.filepath)
        if not file_path.exists():
            current_app.logger.warning(
                "Backup file missing when attempting Discord notification: %s", backup.filename
            )
            return

        # æº–å‚™è¨Šæ¯å…§å®¹
        initiator = backup.creator.username if backup.creator else "ç³»çµ±æ’ç¨‹"
        backup_type_label = "è‡ªå‹•å‚™ä»½" if backup.backup_type == "auto" else "æ‰‹å‹•å‚™ä»½"
        timestamp = backup.created_at.replace(microsecond=0).isoformat() + "Z"

        embed = {
            "title": backup.filename,
            "color": 0x5865F2,
            "fields": [
                {"name": "å‚™ä»½é¡å‹", "value": backup_type_label, "inline": True},
                {"name": "æª”æ¡ˆå¤§å°", "value": backup.get_display_size(), "inline": True},
                {"name": "å»ºç«‹è€…", "value": initiator, "inline": True},
            ],
            "timestamp": timestamp,
        }

        if backup.description:
            embed["description"] = backup.description

        attach_file = backup.file_size <= cls.DISCORD_UPLOAD_LIMIT

        if not attach_file:
            embed["fields"].append(
                {
                    "name": "é™„ä»¶ç‹€æ…‹",
                    "value": "æª”æ¡ˆè¶…é 25MBï¼Œè«‹è‡³ç³»çµ±å¾Œå°ä¸‹è¼‰å‚™ä»½ã€‚",
                    "inline": False,
                }
            )

        payload = {
            "content": f"ğŸ›¡ï¸ {backup_type_label}å®Œæˆï¼š{backup.filename}",
            "embeds": [embed],
        }

        try:
            import requests
        except ModuleNotFoundError:  # pragma: no cover - environment guard
            current_app.logger.error("requests å¥—ä»¶ä¸å­˜åœ¨ï¼Œç„¡æ³•æ¨é€ Discord Webhook")
            return

        try:
            if attach_file:
                with file_path.open("rb") as fh:
                    response = requests.post(
                        normalized_url,
                        data={"payload_json": json.dumps(payload, ensure_ascii=False)},
                        files={"file": (backup.filename, fh, "application/gzip")},
                        timeout=30,
                    )
            else:
                response = requests.post(normalized_url, json=payload, timeout=15)

            if response.status_code not in (200, 204):
                current_app.logger.warning(
                    "Discord webhook å›æ‡‰éé æœŸç‹€æ…‹ %s: %s",
                    response.status_code,
                    response.text[:2000],
                )
        except Exception as exc:  # pragma: no cover - network call
            current_app.logger.error(f"Failed to send backup notification to Discord: {exc}")
